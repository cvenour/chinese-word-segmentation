{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import xgboost as xgb\n",
    "from sklearn.externals import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = '*' #The '*' char will represent an unknown char (i.e. unseen in the training data)\n",
    "start_of_line_token = '^' #The '^' char will represent the start of a sentence\n",
    "end_of_line_token = '$'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_whitespace(somestr):\n",
    "    newstr = re.sub(r\"[\\n\\t\\s]*\", \"\", somestr) #get rid of all whitespace characters including newline\n",
    "    return newstr\n",
    "\n",
    "def find_all_4grams(line_of_chars):\n",
    "    thelist = []\n",
    "    numchars = len(line_of_chars)\n",
    "    num4grams = (numchars - 4) + 1 #given the number of chars in line_of_chars, \n",
    "                                   #this is how many 4grams we'll find within it\n",
    "    for i in range(num4grams):\n",
    "        fourgram = line_of_chars[i:i+4]\n",
    "        thelist.append(fourgram)\n",
    "    \n",
    "    return(thelist)\n",
    "\n",
    "def createLabels(someLine, list_of_4grams):\n",
    "    someLineList = list(someLine)\n",
    "    theLabels = []\n",
    "    ptr = 0\n",
    "    for fourGram in list_of_4grams:\n",
    "        if someLineList[ptr + 2] == ' ':\n",
    "            theLabels.append(1)\n",
    "            someLineList.pop(ptr+2) #get rid of that space we just accounted for\n",
    "        else:\n",
    "            theLabels.append(0)\n",
    "            \n",
    "        ptr = ptr + 1\n",
    "            \n",
    "    return theLabels\n",
    "\n",
    "\n",
    "def findIndexOfBigram(bigram, alphabet, unk_token):\n",
    "    alphabetLength = len(alphabet)\n",
    "    firstChar = bigram[0]\n",
    "    secondChar = bigram[1]\n",
    "    if (firstChar not in alphabet):\n",
    "        firstChar = unk_token\n",
    "    if(secondChar not in alphabet):\n",
    "        secondChar = unk_token\n",
    "    \n",
    "    firstCharMultiplier = alphabet.index(firstChar)\n",
    "    secondCharMultiplier = alphabet.index(secondChar)\n",
    "    uniqueIndexOfBigram = firstCharMultiplier*alphabetLength**1 + secondCharMultiplier*alphabetLength**0\n",
    "    return uniqueIndexOfBigram\n",
    "\n",
    "def findBigramOfIndex(index, alphabet):\n",
    "    alphabetLength = len(alphabet)\n",
    "    row = index // alphabetLength\n",
    "    col = index % alphabetLength\n",
    "    firstchar = alphabet[row]\n",
    "    secondchar = alphabet[col]\n",
    "    return firstchar + secondchar\n",
    "\n",
    "def findIndexOfUnigram(unigram, alphabet, unk_token):\n",
    "    if unigram not in alphabet:\n",
    "        unigram = unk_token\n",
    "    theindex = alphabet.index(unigram)\n",
    "    return theindex\n",
    "\n",
    "def findUnigramOfIndex(index, alphabet):\n",
    "    theunigram = alphabet[index]\n",
    "    return theunigram\n",
    "\n",
    "\n",
    "def create5dStrFeatureVector(fourGram):\n",
    "    f1 = fourGram[0] + fourGram[1]\n",
    "    f2 = fourGram[1]\n",
    "    f3 = fourGram[1] + fourGram[2]\n",
    "    f4 = fourGram[2]\n",
    "    f5 = fourGram[2] + fourGram[3]\n",
    "    featureVector = [f1,f2,f3,f4,f5]\n",
    "    return featureVector\n",
    "\n",
    "def create5dIntFeatureVector(strFeatureVector, alphabet, unk_token):\n",
    "    lengthAlphabet = len(alphabet)\n",
    "    lengthBigramPartOfVector = lengthAlphabet * lengthAlphabet\n",
    "    lengthUnigramPartOfVector = lengthAlphabet\n",
    "    lengthOneHotVector = lengthBigramPartOfVector + lengthUnigramPartOfVector\n",
    "\n",
    "    f1,f2,f3,f4,f5 = strFeatureVector\n",
    "    f1int = findIndexOfBigram(f1, alphabet, unk_token) \n",
    "    f2int = findIndexOfUnigram(f2, alphabet, unk_token) + lengthBigramPartOfVector\n",
    "    f3int = findIndexOfBigram(f3, alphabet, unk_token) \n",
    "    f4int = findIndexOfUnigram(f4, alphabet, unk_token) + lengthBigramPartOfVector\n",
    "    f5int = findIndexOfBigram(f5, alphabet, unk_token) \n",
    "    theVector = [f1int,f2int,f3int,f4int,f5int]\n",
    "    return theVector\n",
    "\n",
    "\n",
    "def loadChineseDataFile(filepath, numLines=None):\n",
    "    encoding = 'big5hkscs'\n",
    "    lines = []\n",
    "    num_errors = 0\n",
    "    #Need the 'rb' argument in the following line. This info, thanks to:\n",
    "    #https://stackoverflow.com/questions/22216076/unicodedecodeerror-utf8-codec-cant-decode-byte-0xa5-in-position-0-invalid-s\n",
    "    linecounter = 1\n",
    "\n",
    "    for line in open(filepath, 'rb'):\n",
    "        try:\n",
    "            decodedLine = line.decode(encoding)\n",
    "            cleanLine = ' '.join(decodedLine.split()) #replace double spaces with a single space\n",
    "            cleanLine2 = re.sub(r\"[\\n]*\", \"\", cleanLine) #get rid of newline character\n",
    "            lines.append(cleanLine2)\n",
    "        except UnicodeDecodeError as e:\n",
    "            num_errors += 1\n",
    "            print(\"error encountered at line\", linecounter)\n",
    "        linecounter = linecounter + 1\n",
    "        \n",
    "        if numLines is not None:\n",
    "            if linecounter > numLines:\n",
    "                break\n",
    "\n",
    "    print('Encountered %d decoding errors.' % num_errors)\n",
    "    # The `lines` list contains strings you can use.\n",
    "    return lines\n",
    "\n",
    "\n",
    "def createAlphabet(lines, unk_token, start_of_line_token, end_of_line_token):\n",
    "\n",
    "    unique_chinese_chars_set = set()\n",
    "\n",
    "    for line in lines:\n",
    "        somestr = re.sub(r\"[\\n\\t\\s]*\", \"\", line) #get rid of whitespace characters\n",
    "        for somechar in somestr:\n",
    "            unique_chinese_chars_set.add(somechar)\n",
    "\n",
    "    #Let's make sure the characters i want to use as special tokens don't already occur in the\n",
    "    #training data\n",
    "\n",
    "    if start_of_line_token in unique_chinese_chars_set:\n",
    "        print(\"error. The char\", start_of_line_token, \"which I use as a special token, appears as\")\n",
    "        print(\"a regular char in the datafile. Do not proceed!\")\n",
    "\n",
    "    if end_of_line_token in unique_chinese_chars_set:\n",
    "        print(\"error. The char\", end_of_line_token, \"which I use as a special token, appears as\")\n",
    "        print(\"a regular char in the datafile. Do not proceed!\")\n",
    "        \n",
    "    chinese_alphabet = [unk_token] + [start_of_line_token] + [end_of_line_token] + list(unique_chinese_chars_set) #we cast the set into a list here\n",
    "    return chinese_alphabet\n",
    "\n",
    "\n",
    "def buildSparseMatrix(lines, start_of_line_token, end_of_line_token, unk_token):\n",
    "    row = np.array([])\n",
    "    col = np.array([])\n",
    "    humanLabelsNumpy = np.array([])\n",
    "    sparseMatrixRowPtr = 0\n",
    "    lineNumber = 0\n",
    "\n",
    "    for someLineOriginal in lines:\n",
    "        if (lineNumber % 1000 == 0):\n",
    "            print(\"processing line\", lineNumber, \"...\")\n",
    "\n",
    "        someLine = start_of_line_token + someLineOriginal #we need a start of line token to form proper feature vector for \n",
    "        #first char in a line\n",
    "\n",
    "        someLine = someLine + end_of_line_token #And we need a end of sentence token to form proper feature vector for\n",
    "        #the last char in a line\n",
    "        #print(\"Here is a line from the file:\", someLine)\n",
    "\n",
    "        someLineSansSpaces = eliminate_whitespace(someLine)\n",
    "        all_the_4grams = []\n",
    "\n",
    "        if len(someLine) >=3:\n",
    "            all_the_4grams = find_all_4grams(someLineSansSpaces)\n",
    "        else:\n",
    "            print(\"An error occurred. Some line in the training file is either blank or has just a\")\n",
    "            print(\"single Chinese character on it without a punctuation mark at the end. This breaks\")\n",
    "            print(\"an important assumption I made about the data.\")\n",
    "            sys.exit()\n",
    "\n",
    "        #print(\"Here are all its 4grams:\", all_the_4grams)\n",
    "        labels = createLabels(someLine, all_the_4grams)\n",
    "        labels = np.asarray(labels)\n",
    "        #print(\"Here are the labels for the 4grams:\", labels)\n",
    "\n",
    "        humanLabelsNumpy = np.concatenate((humanLabelsNumpy, labels))\n",
    "        #print(\"the labels for the 4grams:\", labels)\n",
    "        #print(\"------------------\")\n",
    "        for i in range(len(all_the_4grams)):\n",
    "            ngram = all_the_4grams[i]\n",
    "            #an ngram will look like ABCD for instance\n",
    "            strFeatureVector = create5dStrFeatureVector(ngram)\n",
    "            #print(\"Here is the 5d feature vector string form:\", strFeatureVector)\n",
    "            intFeatureVector = create5dIntFeatureVector(strFeatureVector, chinese_alphabet, unk_token)\n",
    "            #print(\"Here is the 5d feature vector int form:\", intFeatureVector)\n",
    "            #print(\"---------\")\n",
    "            intFeatureVectorNumpy = np.asarray(intFeatureVector)\n",
    "\n",
    "            temp = np.ones((5), dtype=int) * sparseMatrixRowPtr\n",
    "            row = np.concatenate((row, temp)) #which row in the sparse matrix we're building right now\n",
    "            col = np.concatenate((col, intFeatureVectorNumpy)) #the col indexes that will have a 1 in them\n",
    "\n",
    "            sparseMatrixRowPtr = sparseMatrixRowPtr + 1\n",
    "\n",
    "        lineNumber = lineNumber + 1\n",
    "\n",
    "    #print(\"The number of 5d feature vectors built was:\", sparseMatrixRowPtr)\n",
    "    #print(humanLabelsNumpy.shape)\n",
    "    #print(row.shape) #row and col should have same length\n",
    "    #print(col.shape)\n",
    "\n",
    "    #We can now build the sparse matrix\n",
    "    lengthAlphabet = len(chinese_alphabet)\n",
    "    lengthBigramPartOfVector = lengthAlphabet * lengthAlphabet\n",
    "    lengthUnigramPartOfVector = lengthAlphabet\n",
    "    lengthOneHotVector = lengthBigramPartOfVector + lengthUnigramPartOfVector\n",
    "\n",
    "    data = np.ones_like(row) #We're building one-hot vector so non-zero entries in the matrix will have\n",
    "    #values of 1\n",
    "    \n",
    "    sparseOneHotMatrix = csr_matrix((data, (row, col)), shape=(sparseMatrixRowPtr, lengthOneHotVector))\n",
    "    dtrain = xgb.DMatrix(sparseOneHotMatrix,label=humanLabelsNumpy) #need to convert the sparse matrix \n",
    "    #to a dmatrix that the xgboost module understands\n",
    "\n",
    "    return (dtrain,humanLabelsNumpy)\n",
    "\n",
    "\n",
    "def evaluateResults(classifierLabels, humanLabels):\n",
    "    assert len(classifierLabels) == len(humanLabels)\n",
    "    #these two lists have to have the same length\n",
    "    \n",
    "    numPredictions = len(classifierLabels)\n",
    "    truePositives = 0\n",
    "    trueNegatives = 0\n",
    "\n",
    "    falsePositives = 0\n",
    "    falseNegatives = 0\n",
    "    numCorrect = 0\n",
    "    numWrong = 0\n",
    "\n",
    "    for i in range(len(humanLabels)):\n",
    "        if (humanLabels[i]==1):\n",
    "            if (classifierLabels[i] == 1):\n",
    "                truePositives = truePositives + 1\n",
    "                numCorrect = numCorrect + 1\n",
    "            elif(classifierLabels[i] == 0):\n",
    "                falseNegatives = falseNegatives + 1\n",
    "                numWrong = numWrong + 1\n",
    "        elif(humanLabels[i]==0):\n",
    "            if (classifierLabels[i] == 0):\n",
    "                trueNegatives = trueNegatives + 1\n",
    "                numCorrect = numCorrect + 1\n",
    "            elif(classifierLabels[i]==1):\n",
    "                falsePositives = falsePositives + 1\n",
    "                numWrong = numWrong + 1\n",
    "\n",
    "    #print(\"true positives:\", truePositives)\n",
    "    #print(\"false negatives:\", falseNegatives)\n",
    "    #print(\"false positives:\", falsePositives)\n",
    "    #print()\n",
    "\n",
    "    accuracy = numCorrect/numPredictions\n",
    "    precision = truePositives/(truePositives + falsePositives)\n",
    "    recall = truePositives/(truePositives + falseNegatives)\n",
    "    f1 = (2*precision*recall)/(precision+recall)\n",
    "    \n",
    "    return (accuracy,precision,recall,f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's evaluate the trained model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained xgboost decision tree. This will take a few minutes...\n",
      "loading the chinese alphabet it uses...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading trained xgboost decision tree. This will take a few minutes...\")\n",
    "bst = joblib.load('chinese-xgb-model-final.pickle') #load the saved model\n",
    "\n",
    "print(\"Loading the chinese alphabet it uses...\")\n",
    "chinese_alphabet = joblib.load('chinese-alphabet-final.pickle') #load the saved alphabet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered 0 decoding errors.\n",
      "processing line 0 ...\n",
      "processing line 1000 ...\n"
     ]
    }
   ],
   "source": [
    "testLines = loadChineseDataFile(\"./data/test.txt\")\n",
    "(dtest, ytest) = buildSparseMatrix(testLines, start_of_line_token, end_of_line_token, unk_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's make predictions on the test set now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bst.predict(dtest)\n",
    "predsBinary = (preds >= 0.5)\n",
    "predsBinary = predsBinary.astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here are the test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8504468146439896\n",
      "precision: 0.9468858324946886\n",
      "recall: 0.7998488712572023\n",
      "F1 score: 0.867178699436764\n"
     ]
    }
   ],
   "source": [
    "acc,prec,recall,f1 = evaluateResults(predsBinary, ytest)\n",
    "print(\"accuracy:\", acc)\n",
    "print(\"precision:\", prec)\n",
    "print(\"recall:\", recall)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tf2019",
   "language": "python",
   "name": "tf2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
