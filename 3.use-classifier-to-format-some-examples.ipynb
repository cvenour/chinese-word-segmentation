{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import xgboost as xgb\n",
    "from sklearn.externals import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = '*' #The '*' char will represent an unknown char (i.e. unseen in the training data)\n",
    "start_of_line_token = '^' #The '^' char will represent the start of a sentence\n",
    "end_of_line_token = '$'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_whitespace(somestr):\n",
    "    newstr = re.sub(r\"[\\n\\t\\s]*\", \"\", somestr) #get rid of all whitespace characters including newline\n",
    "    return newstr\n",
    "\n",
    "def find_all_4grams(line_of_chars):\n",
    "    thelist = []\n",
    "    numchars = len(line_of_chars)\n",
    "    num4grams = (numchars - 4) + 1 #given the number of chars in line_of_chars, \n",
    "                                   #this is how many 4grams we'll find within it\n",
    "    for i in range(num4grams):\n",
    "        fourgram = line_of_chars[i:i+4]\n",
    "        thelist.append(fourgram)\n",
    "    \n",
    "    return(thelist)\n",
    "\n",
    "def createLabels(someLine, list_of_4grams):\n",
    "    someLineList = list(someLine)\n",
    "    theLabels = []\n",
    "    ptr = 0\n",
    "    for fourGram in list_of_4grams:\n",
    "        if someLineList[ptr + 2] == ' ':\n",
    "            theLabels.append(1)\n",
    "            someLineList.pop(ptr+2) #get rid of that space we just accounted for\n",
    "        else:\n",
    "            theLabels.append(0)\n",
    "            \n",
    "        ptr = ptr + 1\n",
    "            \n",
    "    return theLabels\n",
    "\n",
    "\n",
    "def findIndexOfBigram(bigram, alphabet, unk_token):\n",
    "    alphabetLength = len(alphabet)\n",
    "    firstChar = bigram[0]\n",
    "    secondChar = bigram[1]\n",
    "    if (firstChar not in alphabet):\n",
    "        firstChar = unk_token\n",
    "    if(secondChar not in alphabet):\n",
    "        secondChar = unk_token\n",
    "    \n",
    "    firstCharMultiplier = alphabet.index(firstChar)\n",
    "    secondCharMultiplier = alphabet.index(secondChar)\n",
    "    uniqueIndexOfBigram = firstCharMultiplier*alphabetLength**1 + secondCharMultiplier*alphabetLength**0\n",
    "    return uniqueIndexOfBigram\n",
    "\n",
    "def findBigramOfIndex(index, alphabet):\n",
    "    alphabetLength = len(alphabet)\n",
    "    row = index // alphabetLength\n",
    "    col = index % alphabetLength\n",
    "    firstchar = alphabet[row]\n",
    "    secondchar = alphabet[col]\n",
    "    return firstchar + secondchar\n",
    "\n",
    "def findIndexOfUnigram(unigram, alphabet, unk_token):\n",
    "    if unigram not in alphabet:\n",
    "        unigram = unk_token\n",
    "    theindex = alphabet.index(unigram)\n",
    "    return theindex\n",
    "\n",
    "def findUnigramOfIndex(index, alphabet):\n",
    "    theunigram = alphabet[index]\n",
    "    return theunigram\n",
    "\n",
    "\n",
    "def create5dStrFeatureVector(fourGram):\n",
    "    f1 = fourGram[0] + fourGram[1]\n",
    "    f2 = fourGram[1]\n",
    "    f3 = fourGram[1] + fourGram[2]\n",
    "    f4 = fourGram[2]\n",
    "    f5 = fourGram[2] + fourGram[3]\n",
    "    featureVector = [f1,f2,f3,f4,f5]\n",
    "    return featureVector\n",
    "\n",
    "def create5dIntFeatureVector(strFeatureVector, alphabet, unk_token):\n",
    "    lengthAlphabet = len(alphabet)\n",
    "    lengthBigramPartOfVector = lengthAlphabet * lengthAlphabet\n",
    "    lengthUnigramPartOfVector = lengthAlphabet\n",
    "    lengthOneHotVector = lengthBigramPartOfVector + lengthUnigramPartOfVector\n",
    "\n",
    "    f1,f2,f3,f4,f5 = strFeatureVector\n",
    "    f1int = findIndexOfBigram(f1, alphabet, unk_token) \n",
    "    f2int = findIndexOfUnigram(f2, alphabet, unk_token) + lengthBigramPartOfVector\n",
    "    f3int = findIndexOfBigram(f3, alphabet, unk_token) \n",
    "    f4int = findIndexOfUnigram(f4, alphabet, unk_token) + lengthBigramPartOfVector\n",
    "    f5int = findIndexOfBigram(f5, alphabet, unk_token) \n",
    "    theVector = [f1int,f2int,f3int,f4int,f5int]\n",
    "    return theVector\n",
    "\n",
    "\n",
    "def loadChineseDataFile(filepath, numLines=None):\n",
    "    encoding = 'big5hkscs'\n",
    "    lines = []\n",
    "    num_errors = 0\n",
    "    #Need the 'rb' argument in the following line. This info, thanks to:\n",
    "    #https://stackoverflow.com/questions/22216076/unicodedecodeerror-utf8-codec-cant-decode-byte-0xa5-in-position-0-invalid-s\n",
    "    linecounter = 1\n",
    "\n",
    "    for line in open(filepath, 'rb'):\n",
    "        try:\n",
    "            decodedLine = line.decode(encoding)\n",
    "            cleanLine = ' '.join(decodedLine.split()) #replace double spaces with a single space\n",
    "            cleanLine2 = re.sub(r\"[\\n]*\", \"\", cleanLine) #get rid of newline character\n",
    "            lines.append(cleanLine2)\n",
    "        except UnicodeDecodeError as e:\n",
    "            num_errors += 1\n",
    "            print(\"error encountered at line\", linecounter)\n",
    "        linecounter = linecounter + 1\n",
    "        \n",
    "        if numLines is not None:\n",
    "            if linecounter > numLines:\n",
    "                break\n",
    "\n",
    "    print('Encountered %d decoding errors.' % num_errors)\n",
    "    # The `lines` list contains strings you can use.\n",
    "    return lines\n",
    "\n",
    "\n",
    "def insertSpecialTokens(someLine, start_of_line_token, end_of_line_token):\n",
    "    someLine = start_of_line_token + someLine #we need a start of line token to form proper feature vector for \n",
    "    #first char in a line\n",
    "\n",
    "    someLine = someLine + end_of_line_token #And we need a end of sentence token to form proper feature vector for\n",
    "    #the last char in a line\n",
    "    \n",
    "    return someLine    \n",
    "\n",
    "\n",
    "def evaluateResults(classifierLabels, humanLabels):\n",
    "    assert len(classifierLabels) == len(humanLabels)\n",
    "    #these two lists have to have the same length\n",
    "    \n",
    "    numPredictions = len(classifierLabels)\n",
    "    truePositives = 0\n",
    "    trueNegatives = 0\n",
    "\n",
    "    falsePositives = 0\n",
    "    falseNegatives = 0\n",
    "    numCorrect = 0\n",
    "    numWrong = 0\n",
    "\n",
    "    for i in range(len(humanLabels)):\n",
    "        if (humanLabels[i]==1):\n",
    "            if (classifierLabels[i] == 1):\n",
    "                truePositives = truePositives + 1\n",
    "                numCorrect = numCorrect + 1\n",
    "            elif(classifierLabels[i] == 0):\n",
    "                falseNegatives = falseNegatives + 1\n",
    "                numWrong = numWrong + 1\n",
    "        elif(humanLabels[i]==0):\n",
    "            if (classifierLabels[i] == 0):\n",
    "                trueNegatives = trueNegatives + 1\n",
    "                numCorrect = numCorrect + 1\n",
    "            elif(classifierLabels[i]==1):\n",
    "                falsePositives = falsePositives + 1\n",
    "                numWrong = numWrong + 1\n",
    "\n",
    "    #print(\"true positives:\", truePositives)\n",
    "    #print(\"false negatives:\", falseNegatives)\n",
    "    #print(\"false positives:\", falsePositives)\n",
    "    #print()\n",
    "\n",
    "    accuracy = numCorrect/numPredictions\n",
    "    precision = truePositives/(truePositives + falsePositives)\n",
    "    recall = truePositives/(truePositives + falseNegatives)\n",
    "    \n",
    "    return (accuracy,precision,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePredictions(lines, start_of_line_token, end_of_line_token, unk_token):\n",
    "    row = np.array([])\n",
    "    col = np.array([])\n",
    "    sparseMatrixRowPtr = 0\n",
    "    lineNumber = 0\n",
    "\n",
    "    for someLineOriginal in lines:\n",
    "        if (lineNumber % 1000 == 0):\n",
    "            print(\"processing line\", lineNumber, \"...\")\n",
    "            \n",
    "        someLine = insertSpecialTokens(someLineOriginal, start_of_line_token, end_of_line_token)\n",
    "        all_the_4grams = []\n",
    "\n",
    "        if len(someLine) >=3:\n",
    "            all_the_4grams = find_all_4grams(someLine)\n",
    "        else:\n",
    "            print(\"An error occurred. Some line in the training file is either blank or has just a\")\n",
    "            print(\"single Chinese character on it without a punctuation mark at the end. This breaks\")\n",
    "            print(\"an important assumption I made about the data.\")\n",
    "            sys.exit()\n",
    "\n",
    "        for i in range(len(all_the_4grams)):\n",
    "            ngram = all_the_4grams[i]\n",
    "            #an ngram will look like ABCD for instance\n",
    "            strFeatureVector = create5dStrFeatureVector(ngram)\n",
    "            #print(\"Here is the 5d feature vector string form:\", strFeatureVector)\n",
    "            intFeatureVector = create5dIntFeatureVector(strFeatureVector, chinese_alphabet, unk_token)\n",
    "            #print(\"Here is the 5d feature vector int form:\", intFeatureVector)\n",
    "            #print(\"---------\")\n",
    "            intFeatureVectorNumpy = np.asarray(intFeatureVector)\n",
    "\n",
    "            temp = np.ones((5), dtype=int) * sparseMatrixRowPtr\n",
    "            row = np.concatenate((row, temp)) #which row in the sparse matrix we're building right now\n",
    "            col = np.concatenate((col, intFeatureVectorNumpy)) #the col indexes that will have a 1 in them\n",
    "\n",
    "            sparseMatrixRowPtr = sparseMatrixRowPtr + 1\n",
    "\n",
    "        lineNumber = lineNumber + 1\n",
    "\n",
    "    #print(\"The number of 5d feature vectors built was:\", sparseMatrixRowPtr)\n",
    "    #print(humanLabelsNumpy.shape)\n",
    "    #print(row.shape) #row and col should have same length\n",
    "    #print(col.shape)\n",
    "\n",
    "    #We can now build the sparse matrix\n",
    "    lengthAlphabet = len(chinese_alphabet)\n",
    "    lengthBigramPartOfVector = lengthAlphabet * lengthAlphabet\n",
    "    lengthUnigramPartOfVector = lengthAlphabet\n",
    "    lengthOneHotVector = lengthBigramPartOfVector + lengthUnigramPartOfVector\n",
    "\n",
    "    data = np.ones_like(row) #We're building one-hot vector so non-zero entries in the matrix will have\n",
    "    #values of 1\n",
    "    \n",
    "    sparseOneHotMatrix = csr_matrix((data, (row, col)), shape=(sparseMatrixRowPtr, lengthOneHotVector))\n",
    "    dX = xgb.DMatrix(sparseOneHotMatrix) #need to convert the sparse matrix \n",
    "    #to a dmatrix that the xgboost module understands\n",
    "    \n",
    "    preds = bst.predict(dX)\n",
    "\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatStream(someStreamOfChars, predictions):\n",
    "    numPredictions = len(predictions)\n",
    "    constructStr = \"\"\n",
    "    \n",
    "    for i in range(numPredictions):\n",
    "        constructStr = constructStr + someStreamOfChars[i]\n",
    "        if predictions[i] == 1:\n",
    "            constructStr = constructStr + ' '\n",
    "    \n",
    "    constructStr = constructStr + someStreamOfChars[-1]\n",
    "\n",
    "    return constructStr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how the classifier formats some sentence in the test file (test.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained xgboost decision tree. This will take a few minutes...\n",
      "Loading the chinese alphabet it uses...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading trained xgboost decision tree. This will take a few minutes...\")\n",
    "bst = joblib.load('chinese-xgb-model-final.pickle') #load the saved model\n",
    "\n",
    "print(\"Loading the chinese alphabet it uses...\")\n",
    "chinese_alphabet = joblib.load('chinese-alphabet-final.pickle') #load the saved alphabet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered 0 decoding errors.\n"
     ]
    }
   ],
   "source": [
    "testLines = loadChineseDataFile(\"./data/test.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see how the computer formats the first 10 sentences in the test set.\n",
      "In the pairs of sentences you'll see below, the first sentence will be the way\n",
      "a human formatted the Chinese sentence and the second sentence will be the way\n",
      "the computer formats it:\n",
      "--------------------------\n",
      "Getting rid of spaces in a line from the test set and sending the unbroken stream of\n",
      "chars to the classifier for formatting...\n",
      "processing line 0 ...\n",
      "[1 0 0 1 1 0 0 0 1 1 1]\n",
      "h: 對 基督教 、 天主教徒 而 言 ，\n",
      "c: 對 基督教 、 天主教徒 而 言 ，\n",
      "--------------------------\n",
      "Getting rid of spaces in a line from the test set and sending the unbroken stream of\n",
      "chars to the classifier for formatting...\n",
      "processing line 0 ...\n",
      "[0 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1]\n",
      "h: 聖誕節 不 是 繽紛 的 裝飾 、 聖誕 大餐 或 舞會 狂歡 ，\n",
      "c: 聖誕節 不 是 繽紛 的 裝飾 、 聖誕大餐 或 舞 會 狂歡 ，\n",
      "--------------------------\n",
      "Getting rid of spaces in a line from the test set and sending the unbroken stream of\n",
      "chars to the classifier for formatting...\n",
      "processing line 0 ...\n",
      "[0 1 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1]\n",
      "h: 而是 平安夜 在 教堂 裡 紀念 耶穌基督 的 降生 。\n",
      "c: 而是 平安夜 在 教堂 裡 紀念耶穌基督 的 降生 。\n",
      "--------------------------\n",
      "Getting rid of spaces in a line from the test set and sending the unbroken stream of\n",
      "chars to the classifier for formatting...\n",
      "processing line 0 ...\n",
      "[0 0 1 1 0 1 1 0 0 0 1 1 0 0 0 0 0 1]\n",
      "h: 本 學期 才 成立 為 信望愛社 的 銘傳 學院 團契 ，\n",
      "c: 本學期 才 成立 為 信望愛社 的 銘傳學院團契 ，\n",
      "--------------------------\n",
      "Getting rid of spaces in a line from the test set and sending the unbroken stream of\n",
      "chars to the classifier for formatting...\n",
      "processing line 0 ...\n",
      "[1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1]\n",
      "h: 每 年 選擇 以 詩歌 觀摩會 的 方式 慶祝 聖誕 ，\n",
      "c: 每 年選擇 以 詩歌觀摩會 的 方式慶祝聖誕 ，\n",
      "--------------------------\n",
      "Getting rid of spaces in a line from the test set and sending the unbroken stream of\n",
      "chars to the classifier for formatting...\n",
      "processing line 0 ...\n",
      "[0 0 0 0 1 1 1 0 0 0 1]\n",
      "h: 廿四日 晚上 在 會幕堂 舉行 ，\n",
      "c: 廿四日晚上 在 會 幕堂舉行 ，\n",
      "--------------------------\n",
      "Getting rid of spaces in a line from the test set and sending the unbroken stream of\n",
      "chars to the classifier for formatting...\n",
      "processing line 0 ...\n",
      "[0 1 1 1 0 0 0 1]\n",
      "h: 其中 還 有 戲劇 表演 。\n",
      "c: 其中 還 有 戲劇表演 。\n",
      "--------------------------\n",
      "Getting rid of spaces in a line from the test set and sending the unbroken stream of\n",
      "chars to the classifier for formatting...\n",
      "processing line 0 ...\n",
      "[0 1 0 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1]\n",
      "h: 東吳 大學 團契 今年 與 東吳 教會 在 廿四 晚上 ，\n",
      "c: 東吳 大學 團契今年 與 東吳教 會 在 廿四晚上 ，\n",
      "--------------------------\n",
      "Getting rid of spaces in a line from the test set and sending the unbroken stream of\n",
      "chars to the classifier for formatting...\n",
      "processing line 0 ...\n",
      "[0 0 0 0 1 1]\n",
      "h: 合辦 耶誕 晚會 。\n",
      "c: 合辦耶誕晚 會 。\n",
      "--------------------------\n",
      "Getting rid of spaces in a line from the test set and sending the unbroken stream of\n",
      "chars to the classifier for formatting...\n",
      "processing line 0 ...\n",
      "[0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 0 1]\n",
      "h: 內容 包括 詩歌 獻唱 、 戲劇 表演 及 今年 新 增加 的 布偶戲 。\n",
      "c: 內容包括詩歌獻唱 、 戲劇表演 及 今年 新 增加 的 布偶戲 。\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Let's see how the computer formats the first 10 sentences in the test set.\")\n",
    "print(\"In the pairs of sentences you'll see below, the first sentence will be the way\")\n",
    "print(\"a human formatted the Chinese sentence and the second sentence will be the way\")\n",
    "print(\"the computer formats it:\")\n",
    "print(\"--------------------------\")\n",
    "\n",
    "for someSentence in testLines[0:10]:\n",
    "    print(\"Getting rid of spaces in a line from the test set and sending the unbroken stream of\")\n",
    "    print(\"chars to the classifier for formatting...\")\n",
    "    someSentenceWithoutSpaces = eliminate_whitespace(someSentence)\n",
    "    preds = makePredictions([someSentenceWithoutSpaces], start_of_line_token, end_of_line_token, unk_token)\n",
    "    predsBinary = (preds >= 0.5)\n",
    "    predsBinary = predsBinary.astype(int)\n",
    "    print(predsBinary)\n",
    "    formattedSentence = formatStream(someSentenceWithoutSpaces, predsBinary)\n",
    "\n",
    "    print('h:', someSentence)\n",
    "    print('c:', formattedSentence)\n",
    "    print(\"--------------------------\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python tf2019",
   "language": "python",
   "name": "tf2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
